{"cells":[{"cell_type":"code","execution_count":1,"id":"2e7e8535","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\n","\u001B[0mRequirement already satisfied: nltk in /opt/conda/miniconda3/lib/python3.8/site-packages (3.6.3)\n","Requirement already satisfied: click in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n","Requirement already satisfied: joblib in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk) (2021.4.4)\n","Requirement already satisfied: tqdm in /opt/conda/miniconda3/lib/python3.8/site-packages (from nltk) (4.67.1)\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes\n","!pip install nltk"]},{"cell_type":"code","execution_count":2,"id":"82eb7b8e","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","from operator import add\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":3,"id":"1841f7a2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Jan 12 16:51 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":4,"id":"9895a2b0","metadata":{},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":5,"id":"d29b38c9","metadata":{},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'bucket_324041060' \n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)"]},{"cell_type":"code","execution_count":6,"id":"ec3d1e5c","metadata":{},"outputs":[],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":7,"id":"3766e008","metadata":{},"outputs":[],"source":["from inverted_index_gcp import *\n","\n","InvertedIndex.DIR_NAME = \"title_index\""]},{"cell_type":"code","execution_count":8,"id":"a33bce24","metadata":{},"outputs":[],"source":["full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    name = b.name\n","\n","    if name == 'graphframes.sh':\n","        continue\n","    if name.startswith(('pr/', 'postings_gcp/', 'title_id_dict_parquet/', \"doc_len_dict_parquet/\")):\n","        continue\n","    if not name.endswith('.parquet'):\n","        continue\n","\n","    paths.append(full_path + name)"]},{"cell_type":"code","execution_count":9,"id":"7592a6df","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["parquetFile = spark.read.parquet(*paths)\n","doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n","\n","inverted_title = InvertedIndex()\n","inverted_title.DIR_NAME = InvertedIndex.DIR_NAME"]},{"cell_type":"code","execution_count":10,"id":"0c737bfe","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["num docs (parquet count): 6348910\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 6:============================================>              (3 + 1) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["sample: [Row(title='Foster Air Force Base', id=4045403)]\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# quick sanity\n","print(\"num docs (parquet count):\", parquetFile.count())\n","print(\"sample:\", doc_title_pairs.take(1))"]},{"cell_type":"code","execution_count":11,"id":"9a664456","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Count number of wiki pages\n","N_docs = parquetFile.count()\n","\n","inverted_title.N = N_docs"]},{"cell_type":"code","execution_count":13,"id":"83728f39","metadata":{},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","DIR_NAME = \"title_index\""]},{"cell_type":"code","execution_count":14,"id":"db7e6fc5","metadata":{},"outputs":[],"source":["NUM_BUCKETS = 124\n","\n","def token2bucket_id(token):\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","def word_count(text, id):\n","    ''' Count the frequency of each word in `text` (tf) that is not included in \n","    `all_stopwords` and return entries that will go into our posting lists. \n","    Parameters:\n","    -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","    Returns:\n","    --------\n","    List of tuples\n","    A list of (token, (doc_id, tf)) pairs \n","    for example: [(\"Anarchism\", (12, 5)), ...]'''\n","\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    token_counter = Counter(x for x in tokens if x not in all_stopwords)\n","    res = [(token, (id, tf)) for token, tf in token_counter.items()]\n","    return res"]},{"cell_type":"code","execution_count":15,"id":"5b575800","metadata":{},"outputs":[],"source":["def reduce_word_counts(unsorted_pl):\n","    ''' Returns a sorted posting list by wiki_id.\n","    Parameters:\n","    -----------\n","    unsorted_pl: list of tuples\n","      A list of (wiki_id, tf) tuples \n","    Returns:\n","    --------\n","    list of tuples\n","      A sorted posting list.'''\n","\n","    return list(sorted(unsorted_pl))\n","\n","def calculate_df(postings):\n","    ''' Takes a posting list RDD and calculate the df for each token.\n","    Parameters:\n","    -----------\n","    postings: RDD\n","      An RDD where each element is a (token, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","      An RDD where each element is a (token, df) pair.'''\n","\n","    return postings.map(lambda x: (x[0],len(x[1])))"]},{"cell_type":"code","execution_count":16,"id":"e3e7bc74","metadata":{},"outputs":[],"source":["def partition_postings_and_write(postings):\n","    ''' A function that partitions the posting lists into buckets, writes out \n","    all posting lists in a bucket to disk, and returns the posting locations for \n","    each bucket. Partitioning should be done through the use of `token2bucket` \n","    above. Writing to disk should use the function  `write_a_posting_list`, a \n","    static method implemented in inverted_index_colab.py under the InvertedIndex \n","    class. \n","    Parameters:\n","    -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list) pair.\n","    Returns:\n","    --------\n","    RDD\n","      An RDD where each item is a posting locations dictionary for a bucket. The\n","      posting locations maintain a list for each word of file locations and \n","      offsets its posting list was written to. See `write_a_posting_list` for \n","      more details.'''\n","\n","    buckets_split = postings.map(lambda x: (token2bucket_id(x[0]),x))\n","    buckets_group = buckets_split.groupByKey()\n","    return buckets_group.map(lambda x: InvertedIndex.write_a_posting_list(x, bucket_name))"]},{"cell_type":"code","execution_count":17,"id":"a038763e","metadata":{},"outputs":[],"source":["def count_doc_len(doc_id, text):\n","    '''this functen calculates the document len given the dic id and text '''\n","\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","    num_tokens = len([x for x in tokens if x not in all_stopwords])\n","    return (doc_id,num_tokens)"]},{"cell_type":"code","execution_count":18,"id":"54289f2f","metadata":{},"outputs":[],"source":["# now lets count the frequency of each word in title\n","word_counts_title = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))"]},{"cell_type":"code","execution_count":19,"id":"fe8643f6","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# calculate the length of each title and saves values in DL where {key = doc_id: value = doc_length}\n","len_docs_title = doc_title_pairs.map(lambda x: count_doc_len(x[1], x[0]))\n","len_docs_title = len_docs_title.collectAsMap()\n","inverted_title.DL = len_docs_title"]},{"cell_type":"code","execution_count":20,"id":"47e77353","metadata":{},"outputs":[],"source":["#create postings and sort by doc_id\n","postings_title = word_counts_title.groupByKey().mapValues(reduce_word_counts)"]},{"cell_type":"code","execution_count":21,"id":"a607b916","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# stores total frequency per term as a counter in \"term_total\" feild\n","total_terms_title = postings_title.flatMapValues(lambda x : x).map(lambda x: (x[0],x[1][1])).reduceByKey(add)\n","inverted_title.term_total = total_terms_title.collectAsMap()"]},{"cell_type":"code","execution_count":22,"id":"168e513f","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# filtering postings and calculate df for each term. stores it in title_index.df attribute\n","w2df_title = calculate_df(postings_title)\n","w2df_title_dict = w2df_title.collectAsMap()\n","inverted_title.df = w2df_title_dict"]},{"cell_type":"code","execution_count":23,"id":"edadd250","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# partition posting lists and write out\n","_ = partition_postings_and_write(postings_title).collect()"]},{"cell_type":"code","execution_count":24,"id":"3f88f06c","metadata":{},"outputs":[],"source":["# collect all posting lists locations into one super-set\n","super_posting_locs_title = defaultdict(list)\n","for blob in client.list_blobs(bucket_name, prefix=DIR_NAME):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            super_posting_locs_title[k].extend(v)\n","\n","#stores it in title_index.posting_locs attribute\n","inverted_title.posting_locs = super_posting_locs_title"]},{"cell_type":"code","execution_count":25,"id":"46e7d2a2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying file://title_index.pkl [Content-Type=application/octet-stream]...\n","/ [1 files][132.9 MiB/132.9 MiB]                                                \n","Operation completed over 1 objects/132.9 MiB.                                    \n"]}],"source":["# write the global stats out\n","inverted_title.write_index('.', 'title_index')\n","# upload to gs\n","index_src = \"title_index.pkl\"\n","index_dst = f'gs://{bucket_name}/title_index/{index_src}'\n","!gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":26,"id":"36d65cdf","metadata":{},"outputs":[{"data":{"text/plain":["inverted_index_gcp.InvertedIndex"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["import pickle\n","\n","with open(\"title_index.pkl\", \"rb\") as f:\n","    index = pickle.load(f)\n","\n","type(index)\n"]},{"cell_type":"code","execution_count":28,"id":"11f17138","metadata":{},"outputs":[{"data":{"text/plain":["[('lindström', 84),\n"," ('1983', 3192),\n"," ('wwf', 77),\n"," ('squarefoot', 1),\n"," ('frankie', 343),\n"," ('los', 4388),\n"," ('prosser', 90),\n"," ('origin', 461),\n"," ('fevey', 1),\n"," ('café', 402)]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["index.N            \n","len(index.df)      \n","list(index.df.items())[:10]  \n"]},{"cell_type":"code","execution_count":29,"id":"d59a8326","metadata":{},"outputs":[{"data":{"text/plain":["1786"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["index.df[\"computer\"]        \n","index.term_total[\"computer\"]  "]},{"cell_type":"code","execution_count":32,"id":"6a61aa25","metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['df', 'term_total', 'posting_locs', 'DL', 'tf_term_doc', 'N', 'DIR_NAME'])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["index.__dict__.keys()"]},{"cell_type":"code","execution_count":33,"id":"21d72aed","metadata":{},"outputs":[{"data":{"text/plain":["1774249"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["len(index.posting_locs)"]},{"cell_type":"code","execution_count":34,"id":"c5b4b68b","metadata":{},"outputs":[{"data":{"text/plain":["[('kajevski', [('0_000.bin', 0)]),\n"," ('ayats', [('0_000.bin', 6)]),\n"," ('groenhorst', [('0_000.bin', 18)]),\n"," ('okyay', [('0_000.bin', 24)]),\n"," ('raparo', [('0_000.bin', 66)])]"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["list(index.posting_locs.items())[:5]"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}