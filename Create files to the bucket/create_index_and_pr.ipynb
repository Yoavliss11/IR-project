{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac36d3a",
      "metadata": {
        "id": "5ac36d3a",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Worker_Count",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "cf88b954-f39a-412a-d87e-660833e735b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE  SCHEDULED_STOP\n",
            "cluster-0016  GCE       2                                             RUNNING  us-central1-a\n"
          ]
        }
      ],
      "source": [
        "# if the following command generates an error, you probably didn't enable\n",
        "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
        "# under Manage Security â†’ Project Access when setting up the cluster\n",
        "!gcloud dataproc clusters list --region us-central1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51cf86c5",
      "metadata": {
        "id": "51cf86c5"
      },
      "source": [
        "# Imports & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf199e6a",
      "metadata": {
        "id": "bf199e6a",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Setup",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "fc0e315d-21e9-411d-d69c-5b97e4e5d629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f56ecd",
      "metadata": {
        "id": "d8f56ecd",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-Imports",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "a24aa24b-aa75-4823-83ca-1d7deef0f0de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pyspark\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from google.cloud import storage\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import SparkSession\n",
        "from graphframes import *\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, avg\n",
        "from inverted_index_gcp import InvertedIndex\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38a897f2",
      "metadata": {
        "id": "38a897f2",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-jar",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "8f93a7ec-71e0-49c1-fc81-9af385849a90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 247882 Jan  5 16:58 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
          ]
        }
      ],
      "source": [
        "# if nothing prints here you forgot to include the initialization script when starting the cluster\n",
        "!ls -l /usr/lib/spark/jars/graph*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "980e62a5",
      "metadata": {
        "id": "980e62a5",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-bucket_name",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Put your bucket name below and make sure you can access it without an error\n",
        "bucket_name = 'yoav-wiki-bucket-123'\n",
        "full_path = f\"gs://{bucket_name}/\"\n",
        "paths = []\n",
        "\n",
        "client = storage.Client()\n",
        "blobs = client.list_blobs(bucket_name)\n",
        "\n",
        "for b in blobs:\n",
        "    name = b.name\n",
        "\n",
        "    if name == 'graphframes.sh':\n",
        "        continue\n",
        "    if name.startswith(('pr/', 'postings_gcp/', 'title_id_dict_parquet/')):\n",
        "        continue\n",
        "    if not name.endswith('.parquet'):\n",
        "        continue\n",
        "\n",
        "    paths.append(full_path + name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac891c2",
      "metadata": {
        "id": "cac891c2"
      },
      "source": [
        "***GCP setup is complete!***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582c3f5e",
      "metadata": {
        "id": "582c3f5e"
      },
      "source": [
        "# Building an inverted index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "481f2044",
      "metadata": {
        "id": "481f2044"
      },
      "source": [
        "Here, we read the entire corpus to an rdd, directly from Google Storage Bucket and use your code from Colab to construct an inverted index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4c523e7",
      "metadata": {
        "id": "e4c523e7",
        "scrolled": false,
        "outputId": "f3ff7028-dc6f-4b5a-b3b9-fc8bfb520dd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "parquetFile = spark.read.parquet(*paths)\n",
        "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "018b86c4",
      "metadata": {
        "id": "018b86c4",
        "outputId": "70b8f98b-bcd4-42fd-fa9a-6acc8bc5c0bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Titles parquet saved to: gs://yoav-wiki-bucket-123/title_id_dict_parquet\n"
          ]
        }
      ],
      "source": [
        "# Paths for parquets\n",
        "TITLES_PATH  = f\"gs://{bucket_name}/title_id_dict_parquet\"\n",
        "DOC_LEN_PATH = f\"gs://{bucket_name}/doc_len_dict_parquet\"\n",
        "\n",
        "# Tokenizer setup\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "\n",
        "def _doc_len(text: str) -> int:\n",
        "    tokens = [m.group() for m in RE_WORD.finditer(text.lower())]\n",
        "    tokens = [t for t in tokens if t not in all_stopwords]\n",
        "    return len(tokens)\n",
        "\n",
        "# Save titles parquet (id -> title)\n",
        "(parquetFile\n",
        "  .select(col(\"id\").cast(\"long\").alias(\"id\"), col(\"title\"))\n",
        "  .write\n",
        "  .mode(\"overwrite\")\n",
        "  .parquet(TITLES_PATH))\n",
        "\n",
        "print(\"Titles parquet saved to:\", TITLES_PATH)\n",
        "\n",
        "# Save doc length parquet (id -> doc_len)\n",
        "doc_len_rdd = doc_text_pairs.map(lambda x: Row(id=int(x[1]), doc_len=_doc_len(x[0])))\n",
        "doc_len_df = spark.createDataFrame(doc_len_rdd)\n",
        "\n",
        "(doc_len_df\n",
        "  .write\n",
        "  .mode(\"overwrite\")\n",
        "  .parquet(DOC_LEN_PATH))\n",
        "\n",
        "print(\"Doc-len parquet saved to:\", DOC_LEN_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4512087e",
      "metadata": {
        "id": "4512087e",
        "outputId": "3cdad227-62b5-499d-bd11-db9e01167c7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/_SUCCESS\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00000-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00002-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00005-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00007-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00009-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00010-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00012-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00013-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00015-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00016-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00017-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00018-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00019-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00021-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00022-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00024-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00025-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00026-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00027-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00028-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00030-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00031-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00032-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00033-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00034-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00035-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00036-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00037-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00038-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00039-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00040-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00041-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00042-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00043-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00044-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00045-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00046-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00047-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00048-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00049-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00050-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00051-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00052-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00053-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00054-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00055-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00057-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00060-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00063-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00067-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00071-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00075-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00079-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00083-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00110-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00111-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00115-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00118-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00120-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00121-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n",
            "gs://yoav-wiki-bucket-123/title_id_dict_parquet/part-00122-5a188c2a-fdac-46f7-af52-7ad834324e41-c000.snappy.parquet\r\n"
          ]
        }
      ],
      "source": [
        "!gsutil ls gs://{bucket_name}/title_id_dict_parquet\n",
        "!gsutil ls gs://{bucket_name}/doc_len_dict_parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d7e2971",
      "metadata": {
        "id": "0d7e2971"
      },
      "source": [
        "We will count the number of pages to make sure we are looking at the entire corpus. The number of pages should be more than 6M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82881fbf",
      "metadata": {
        "id": "82881fbf",
        "outputId": "56f02e11-6c25-4295-885e-c7edec067ca5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6348910"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Count number of wiki pages\n",
        "parquetFile.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "701811af",
      "metadata": {
        "id": "701811af"
      },
      "source": [
        "Let's import the inverted index module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121fe102",
      "metadata": {
        "id": "121fe102",
        "outputId": "327fe81b-80f4-4b3a-8894-e74720d92e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inverted_index_gcp.py\r\n"
          ]
        }
      ],
      "source": [
        "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
        "%cd -q /home/dataproc\n",
        "!ls inverted_index_gcp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c101a8",
      "metadata": {
        "id": "57c101a8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# adding our python module to the cluster\n",
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ad8fea",
      "metadata": {
        "id": "f3ad8fea",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-token2bucket",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "NUM_BUCKETS = 124\n",
        "\n",
        "def token2bucket_id(token):\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def word_count(text, id):\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "\n",
        "  filtered_words = []\n",
        "  for word in tokens:\n",
        "    if word not in all_stopwords:\n",
        "      filtered_words.append(word)\n",
        "\n",
        "  counts = Counter()\n",
        "  for word in filtered_words:\n",
        "    counts[word] += 1\n",
        "\n",
        "  result = []\n",
        "  for word, tf in counts.items():\n",
        "    result.append((word, (id, tf)))\n",
        "\n",
        "  return result\n",
        "\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "  return sorted(unsorted_pl, key=lambda x: x[0])\n",
        "\n",
        "def calculate_df(postings):\n",
        "  return postings.mapValues(lambda pl: len(pl)) # compute df by counting how many doc_ids appear in its posting list\n",
        "\n",
        "def partition_postings_and_write(postings):\n",
        "  bucketed = postings.map(lambda x: (token2bucket_id(x[0]), (x[0], x[1]))) # (token, pl) -> (bucket_id, (token, pl))\n",
        "  grouped = bucketed.groupByKey()  # group all (token, pl) pairs that belong to the same backet_id\n",
        "\n",
        "  # For each bucket - write all its posting lists to disk and get back a dict {token:[(file_name, offset),...],...}\n",
        "  result = grouped.map(\n",
        "      lambda bucket_data:\n",
        "                       InvertedIndex.write_a_posting_list(\n",
        "                           (bucket_data[0], list(bucket_data[1])),\n",
        "                           bucket_name))\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c8764e",
      "metadata": {
        "id": "55c8764e",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-index_construction",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "67bc64d2-6b3c-41a7-c0a5-b93f972f6138"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# time the index creation time\n",
        "t_start = time()\n",
        "# word counts map\n",
        "word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "# filtering postings and calculate df\n",
        "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
        "w2df = calculate_df(postings_filtered)\n",
        "w2df_dict = w2df.collectAsMap()\n",
        "# partition posting lists and write out\n",
        "_ = partition_postings_and_write(postings_filtered).collect()\n",
        "index_const_time = time() - t_start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dbc0e14",
      "metadata": {
        "id": "3dbc0e14",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-index_const_time",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# test index construction time\n",
        "assert index_const_time < 60*120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab3296f4",
      "metadata": {
        "id": "ab3296f4",
        "nbgrader": {
          "grade": true,
          "grade_id": "collect-posting",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# collect all posting lists locations into one super-set\n",
        "super_posting_locs = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
        "  if not blob.name.endswith(\"pickle\"):\n",
        "    continue\n",
        "  with blob.open(\"rb\") as f:\n",
        "    posting_locs = pickle.load(f)\n",
        "    for k, v in posting_locs.items():\n",
        "      super_posting_locs[k].extend(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f66e3a",
      "metadata": {
        "id": "f6f66e3a"
      },
      "source": [
        "Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5d2cfb6",
      "metadata": {
        "id": "a5d2cfb6",
        "outputId": "df2b2692-f9cb-471e-ad73-562959860b04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying file://index.pkl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][ 18.4 MiB/ 18.4 MiB]                                                \n",
            "Operation completed over 1 objects/18.4 MiB.                                     \n"
          ]
        }
      ],
      "source": [
        "# Create inverted index instance\n",
        "inverted = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "inverted.posting_locs = super_posting_locs\n",
        "# Add the token - df dictionary to the inverted index\n",
        "inverted.df = w2df_dict\n",
        "# write the global stats out\n",
        "inverted.write_index('.', 'index')\n",
        "# upload to gs\n",
        "index_src = \"index.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f880d59",
      "metadata": {
        "id": "8f880d59",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-index_dst_size",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "f1df8f25-936f-4633-e935-39504055b1c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 18.45 MiB  2026-01-05T19:06:56Z  gs://yoav-wiki-bucket-123/postings_gcp/index.pkl\r\n",
            "TOTAL: 1 objects, 19341984 bytes (18.45 MiB)\r\n"
          ]
        }
      ],
      "source": [
        "!gsutil ls -lh $index_dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "856f63be",
      "metadata": {
        "id": "856f63be",
        "outputId": "197a91e8-f069-48e9-ea87-db1ca033c48b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing with 50 terms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 20:======================================>                   (2 + 1) / 3]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Titles coverage for corpus sample: 50 / 50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Config\n",
        "bucket_name = \"yoav-wiki-bucket-123\"\n",
        "TITLES_PATH  = f\"gs://{bucket_name}/title_id_dict_parquet\"\n",
        "DOC_LEN_PATH = f\"gs://{bucket_name}/doc_len_dict_parquet\"\n",
        "\n",
        "# Load titles\n",
        "titles_df = (\n",
        "    spark.read.parquet(TITLES_PATH)\n",
        "    .select(col(\"id\").cast(\"long\").alias(\"id\"))\n",
        ")\n",
        "\n",
        "# Load doc lengths\n",
        "doc_len_df = (\n",
        "    spark.read.parquet(DOC_LEN_PATH)\n",
        "    .select(col(\"id\").cast(\"long\").alias(\"id\"),\n",
        "            col(\"doc_len\").cast(\"int\").alias(\"doc_len\"))\n",
        ")\n",
        "\n",
        "doc_len_dict = doc_len_df.rdd.map(lambda r: (r[\"id\"], r[\"doc_len\"])).collectAsMap()\n",
        "avgdl = doc_len_df.agg(avg(\"doc_len\")).first()[0]\n",
        "\n",
        "print(\"titles_df loaded\")\n",
        "print(\"doc_len_dict size:\", len(doc_len_dict))\n",
        "print(\"avgdl:\", avgdl)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check: titles coverage\n",
        "sample_terms = list(inverted.df.keys())[:50]\n",
        "corpus_ids = [int(r[\"id\"]) for r in parquetFile.select(\"id\").limit(50).collect()]\n",
        "found = titles_df.where(col(\"id\").isin(corpus_ids)).count()"
      ],
      "metadata": {
        "id": "pyBdiTJ37NUv"
      },
      "id": "pyBdiTJ37NUv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save TITLE_ID_DICT as pkl\n",
        "PARQUET_PATH = \"gs://yoav-wiki-bucket-123/title_id_dict_parquet\"  # path to parquet folder in GCS\n",
        "df = spark.read.parquet(PARQUET_PATH)   # read all parquet files in the folder\n",
        "data = df.collect()\n",
        "\n",
        "with open(\"title_id_dict.pkl\", \"wb\") as f:\n",
        "    pickle.dump(data, f)\n",
        "\n",
        "BUCKET_DST = \"gs://yoav-wiki-bucket-123/title_id_dict.pkl\"\n",
        "!gsutil cp title_id.pkl $BUCKET_DST"
      ],
      "metadata": {
        "id": "UK_QEtptKQOQ"
      },
      "id": "UK_QEtptKQOQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c52dee14",
      "metadata": {
        "id": "c52dee14",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2a6d655c112e79c5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# PageRank"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0875c6bd",
      "metadata": {
        "id": "0875c6bd",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2fee4bc8d83c1e2a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Compute PageRank for the entire English Wikipedia. Use your implementation for `generate_graph` function from Colab below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31a516e2",
      "metadata": {
        "id": "31a516e2"
      },
      "outputs": [],
      "source": [
        "def generate_graph(pages):\n",
        "  # Edges - (src_id, dst_id) for every link in anchor_text\n",
        "  edges = pages.flatMap(lambda row: ([] if row.anchor_text is None  # no link = no edge\n",
        "                                     else [(row.id, dest.id) for dest in row.anchor_text if dest.id is not None])).distinct().cache()  # link = edge\n",
        "\n",
        "  # Vertices - all unique page ids that appear as source or as destination\n",
        "  vertex_ids = edges.flatMap(lambda e: [e[0], e[1]]).distinct()\n",
        "  vertices = vertex_ids.map(lambda vid: (vid,)).cache() # convert id -> (id,)\n",
        "\n",
        "  return edges, vertices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc05ba3",
      "metadata": {
        "id": "6bc05ba3",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-PageRank",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "373fef76-d73b-4b86-fe0a-4e433658165b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 340:====================================================>(199 + 1) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+\n",
            "|     id|          pagerank|\n",
            "+-------+------------------+\n",
            "|3434750| 9913.728782160775|\n",
            "|  10568| 5385.349263642041|\n",
            "|  32927| 5282.081575765278|\n",
            "|  30680| 5128.233709604119|\n",
            "|5843419| 4957.567686263868|\n",
            "|  68253| 4769.278265355158|\n",
            "|  31717|4486.3501805483065|\n",
            "|  11867| 4146.414650912772|\n",
            "|  14533|3996.4664408855037|\n",
            "| 645042|3531.6270898037437|\n",
            "|  17867|3246.0983906041415|\n",
            "|5042916| 2991.945739166177|\n",
            "|4689264| 2982.324883041747|\n",
            "|  14532|2934.7468292031704|\n",
            "|  25391| 2903.546223513398|\n",
            "|   5405|2891.4163291546365|\n",
            "|4764461|2834.3669873326603|\n",
            "|  15573|2783.8651181588366|\n",
            "|   9316|2782.0396464137702|\n",
            "|8569916| 2775.286191840015|\n",
            "+-------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "t_start = time()\n",
        "pages_links = spark.read.parquet(\"gs://bucket_324041060/multistream*\").select(\"id\", \"anchor_text\").rdd\n",
        "# construct the graph\n",
        "edges, vertices = generate_graph(pages_links)\n",
        "# compute PageRank\n",
        "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
        "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
        "g = GraphFrame(verticesDF, edgesDF)\n",
        "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
        "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
        "pr = pr.sort(col('pagerank').desc())\n",
        "pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")\n",
        "pr_time = time() - t_start\n",
        "pr.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7717604",
      "metadata": {
        "id": "f7717604",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-PageRank_time",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# test that PageRank computaion took less than 1 hour\n",
        "assert pr_time < 60*120"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}